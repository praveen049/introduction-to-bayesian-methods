{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Introduction to Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "* What is Bayesian inference ?\n",
    "* Background\n",
    "* Comparison with Frequentist approach\n",
    "* Challenges in practical application\n",
    "* Tools available\n",
    "* Example-1\n",
    "* Example-2\n",
    "* Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Bayesian inference ?\n",
    "\n",
    "### Inference involves :\n",
    "\n",
    "* Some **unknown quantities** about which we are interested in learning or testing. We call these parameters.\n",
    "* Some **data** which have been observed, and hopefully contain information about these parameters.\n",
    "* One or more **models** that relate the data to the parameters, and is the instrument that is used to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Inference :\n",
    "\n",
    "> Practical methods for making inferences from data using probability models for quantities we observe and about which we wish to learn.\n",
    "*-- Gelman et al. 2013*\n",
    "\n",
    "A Bayesian model is described by parameters, uncertainty in those parameters is described using probability distributions.\n",
    "\n",
    "All conclusions from Bayesian statistical procedures are stated in terms of **probability statements**\n",
    "\n",
    "![prob model](images/prob_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background \n",
    "\n",
    "As the name suggests, Bayesian inference and Bayesian method was first thought by Thomas Bayes. He was analyzing a problem: **Can we use evidence about the natural world around us to make rational conclusion about the existence of God.**\n",
    "\n",
    "And he came up with a principle (in ~1763):  \n",
    "<div style=\"font-size: 120%;\">\n",
    "We modify our initial guess with objective new information to get a new and improved belief, which in turn carries a commitment to update that belief each time a new piece of information arrives.\n",
    "</div>\n",
    "\n",
    "Bayes' system was: **Initial Belief + New Data -> Improved Belief**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Laplace rederived it (in 1774) and called it **\"Probability of Causes\"**. We used this technique to calculate the mass of Saturn, amongst other things.\n",
    "\n",
    "He came up with this formula:\n",
    "\n",
    "\\begin{equation} \\Pr(M~|~D,I) = \\dfrac{\\Pr(D~|~M,I)\\times\\Pr(M ~|~I)}{\\Pr(D~|~I)} \\label{eqn:lapbayes} \\end{equation} \n",
    "\n",
    "where $M$ is the mass of the Saturn ,  \n",
    "       $D$ is data from various measurements of planetary orbits,    \n",
    " and $I$ is existing background information, e.g., Newton‚Äôs laws of celestial mechanics\n",
    "\n",
    "His estimate is just **0.6%** off of the currently accepted value for  ùëÄ\n",
    "\n",
    "\n",
    "This is basis of the formula that we studied in high school\n",
    "\n",
    "\n",
    "\\\\[P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}\\\\]\n",
    "\n",
    "\n",
    "* P(B|A) is a conditional probability: the probability of event B occurring given that A has occured.\n",
    "* P(A|B) is also a conditional probability: the probability of event A occuring given that B has occured\n",
    "* P(A), P(B) are the probabilities of A and B occuring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian vs Frequentist Statistics: *What's the difference?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Frequentist World View\n",
    "\n",
    "![Fisher](images/fisher.png)\n",
    "\n",
    "- The **data** that have been observed are considered **random**, because they are realizations of random processes, and hence will vary each time one goes to observe the system.\n",
    "- Model **parameters** are considered **fixed**. A parameter's true value is uknown and fixed, and so we *condition* on them.\n",
    "\n",
    "In mathematical notation, this implies a (very) general model of the following form:\n",
    "\n",
    "<div style=\"font-size:35px\">\n",
    "\\\\[f(y | \\theta)\\\\]\n",
    "</div>\n",
    "\n",
    "Here, the model \\\\(f\\\\) accepts data values \\\\(y\\\\) as an argument, conditional on particular values of \\\\(\\theta\\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Bayesian World View\n",
    "\n",
    "![Bayes](images/bayes.png)\n",
    "\n",
    "- Data are considered **fixed**. They used to be random, but once they were written into your lab notebook/spreadsheet/IPython notebook they do not change.\n",
    "- Model parameters themselves may not be random, but Bayesians use probability distribtutions to describe their uncertainty in parameter values, and are therefore treated as **random**.\n",
    "\n",
    "This implies the following form:\n",
    "\n",
    "<div style=\"font-size:35px\">\n",
    "\\\\[p(\\theta | y)\\\\]\n",
    "</div>\n",
    "\n",
    "This formulation used to be referred to as ***inverse probability***, because it infers from observations to parameters, or from effects to causes.\n",
    "\n",
    "Bayesians do not seek new estimators for every estimation problem they encounter. There is only one estimator for Bayesian inference: **Bayes' Formula**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Formula\n",
    "\n",
    "Bayesian inference is based on Bayes Formula\n",
    "![bayes formula](images/bayes_formula.png)\n",
    "\n",
    "The equation expresses how our belief about the value of \\\\(\\theta\\\\), as expressed by the **prior distribution** \\\\(P(\\theta)\\\\) is reallocated following the observation of the data \\\\(y\\\\).\n",
    "\n",
    "The innocuous denominator \\\\(P(y)\\\\) usually cannot be computed directly, and is actually the expression in the numerator, integrated over all \\\\(\\theta\\\\):\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "\\\\[Pr(\\theta|y) = \\frac{Pr(y|\\theta)Pr(\\theta)}{\\int Pr(y|\\theta)Pr(\\theta) d\\theta}\\\\]\n",
    "</div>\n",
    "\n",
    "The intractability of this integral is one of the factors that has contributed to the under-utilization of Bayesian methods by statisticians.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Priors\n",
    "\n",
    "Once considered a controversial aspect of Bayesian analysis, the prior distribution characterizes what is known about an unknown quantity before observing the data from the present study. Thus, it represents the information state of that parameter. It can be used to reflect the information obtained in previous studies, to constrain the parameter to plausible values, or to represent the population of possible parameter values, of which the current study's parameter value can be considered a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood functions\n",
    "\n",
    "The likelihood represents the information in the observed data, and is used to update prior distributions to posterior distributions. This updating of belief is justified becuase of the **likelihood principle**, which states:\n",
    "\n",
    "> Following observation of \\\\(y\\\\), the likelihood \\\\(L(\\theta|y)\\\\) contains all experimental information from \\\\(y\\\\) about the unknown \\\\(\\theta\\\\).\n",
    "\n",
    "Bayesian analysis satisfies the likelihood principle because the posterior distribution's dependence on the data is **only through the likelihood**. In comparison, most frequentist inference procedures violate the likelihood principle, because inference will depend on the design of the trial or experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges\n",
    "\n",
    "Even though Bayesian approach seems a very natual way of dealing with problems, it has historically had two main challenges\n",
    "\n",
    "* Priors and the subjectivity associated with the Priors\n",
    "\n",
    "* Calculating the normalizing constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem with the priors\n",
    "\n",
    "Historically priors have been a problem. The main criticism for Bayesian inference and Priors has been\n",
    "\n",
    "* They are subjective and we come up with them before seeing the data. Negates: 'Lets the data speak for itself'\n",
    "\n",
    "* Probability as 'The degree of Uncertainty' was not the common view. Instead the frequentist view of probabilty as the 'Ratio of Occurences' held sway.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem of computation\n",
    "\n",
    "\n",
    "The innocuous denominator \\\\(P(y)\\\\) usually cannot be computed directly, and is actually the expression in the numerator, integrated over all \\\\(\\theta\\\\).\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "\\\\[{\\int Pr(y|\\theta)Pr(\\theta) d\\theta}\\\\]\n",
    "</div>\n",
    "\n",
    "If we have n parameters we need to integrated over n variables and it can be solved analytically and this prevented Bayesian Inference to be applied to any complex real work problem.\n",
    "\n",
    "The solution for this was Sampling and fast computers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Monte Carlo sampling techniques\n",
    "\n",
    "What the method produces for us is an approximation of the posterior distribution, p(Œ∏|D), in the form of a large number of Œ∏ values sampled from that distribution. This heap of representative Œ∏ values can be used to estimate the central tendency of the posterior, its highest density interval (HDI), etc. The posterior distribution is estimated by randomly generating a lot of values from it\n",
    "\n",
    "The goal in Bayesian inference is to get an accurate representation of the posterior distribution. One way to do that is to sample a large number of representative points from the posterior. The question then becomes this: How can we sample a large number of representative values from a distribution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Metropolis\n",
    "\n",
    "The simplest sampling solution is the **Metropolis-Hastings** algorithm\n",
    "\n",
    "\n",
    "One of the simplest and most flexible algorithms for generating reversible Markov chains is the Metropolis-Hastings algorithm. Since we cannot sample directly from the (unknown) posterior distribution, this algorithm employs an **auxilliary distribution** that is easy to sample from. These samples generate candidate state transitions, which are accepted or rejected probabilistically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Metropolis](images/Metropolis.png)\n",
    "\n",
    "Consider a simple Metropolis-Hastings algorithm for a single parameter, $\\theta$. We will use a well-known statistical distribution like Normal distribution to produce candidate variables $q_t(\\theta^{\\prime} | \\theta)$. Each generated value, $\\theta^{\\prime}$, is a *possible* next value for\n",
    "$\\theta$ at step $t+1$. \n",
    "\n",
    "Whether or not $\\theta^{\\prime}$ is accepted depends on the relative probability of the new value versus the current value, weighted by the probabilities of the two values under the proposal distribution: \n",
    "\n",
    "$$a(\\theta^{\\prime},\\theta) = \\frac{q_t(\\theta^{\\prime} | \\theta) \\pi(\\theta^{\\prime})}{q_t(\\theta | \\theta^{\\prime}) \\pi(\\theta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is the **acceptance ratio**, and is used as a threshold value for a uniform random draw that determines acceptance:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \n",
    "\\begin{cases}\n",
    "\\theta^{\\prime}  & \\text{with prob. } p = \\min(a(\\theta^{\\prime},\\theta^{(t)}),1)\\cr\n",
    "\\theta^{(t)}  & \\text{with prob } 1 - p\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This transition kernel implies that movement is not guaranteed at every step. It only occurs if the suggested transition is likely based on the acceptance ratio.\n",
    "\n",
    "A single iteration of the Metropolis-Hastings algorithm proceeds as follows:\n",
    "\n",
    "1.  Sample $\\theta^{\\prime}$ from $q(\\theta^{\\prime} | \\theta^{(t)})$.\n",
    "\n",
    "2.  Generate a Uniform[0,1] random variate $u$.\n",
    "\n",
    "3.  If $a(\\theta^{\\prime},\\theta) > u$ then\n",
    "    $\\theta^{(t+1)} = \\theta^{\\prime}$, otherwise\n",
    "    $\\theta^{(t+1)} = \\theta^{(t)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NUTS (No UTurns Sampler)\n",
    "\n",
    "For a future session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Libraries \n",
    "\n",
    "There are a bunch of libraries available which can insolving problem using Bayesian inference.\n",
    "\n",
    "* WinBugs  \n",
    "* **Stan** and pyStan   (NOTE: TimeSeries libary Prophet from Facebook is based on Stan)\n",
    "* Edward   \n",
    "* **Pymc3**    \n",
    "* Pymc4 (To be released)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Bayesian inference provides a powerful methods to solve problem with data, inference and prediction.\n",
    "\n",
    "In our context, Bayesian tools can be effective for use cases where\n",
    "\n",
    "* There is a large domain expert knowledge which can be used as prior\n",
    "* There is a inflow for new labelled data which can be used to update the model to create new posterior\n",
    "* There is a need to interpret and reason the model predictions\n",
    "* Sufficient HW resources are avaiable. Bayesian tools are more comptutionally than classical ML algorithms but less so than Deep Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "References: \n",
    "\n",
    "* [Doing Bayesian Data Analysis](http://www.r-5.org/files/books/computers/algo-list/statistics/data-mining/John_K_Kruschke-Doing_Bayesian_Data_Analysis-EN.pdf)\n",
    "\n",
    "* [Introduction to Hamiltonian Monte Carlo](https://arxiv.org/pdf/1701.02434.pdf)\n",
    "\n",
    "* [History of Bayes Theorem](https://www.lesswrong.com/posts/RTt59BtFLqQbsSiqd/a-history-of-bayes-theorem)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
